# Attention Mechanisms from Scratch

## About

This repository contains implementations and resources related to "Attention Mechanisms in Natural Language Processing (NLP)". It serves as a comprehensive guide for those looking to understand and implement attention models from the ground up. Originating from the Coursera course on "Attention models in NLP", the content here provides practical Jupyter Notebook examples and insights into the workings of attention mechanisms, a pivotal concept in modern deep learning architectures for NLP tasks.

The repository is based on the course called "Attention models in NLP" from Coursera. Whether you're a student, researcher, or NLP enthusiast, this repository offers valuable insights into the world of attention, helping you grasp the intricacies of this powerful mechanism.

## Resources
- [01-NeuralMachineTranslation](https://github.com/Oriolac/attention-mechanisms/tree/main/01-NeuralMachineTranslation)

## Topics
- NLP
- Natural Language Processing
- Transformers
- Attention Mechanism
- Transformer Architecture